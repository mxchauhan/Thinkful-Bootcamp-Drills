{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit 3 - Challenge: what model can answer this question?.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mxchauhan/Thinkful-Bootcamp-Drills/blob/master/Unit_3_Challenge_what_model_can_answer_this_question%3F.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MkO4g2zOL-x8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.**\n",
        "\n",
        "Linear regression - the variable we are trying to predict, time, is continuous.  We can take features of sprinters from the previous Olympics (height, weight, country, etc.) and then find the best fitting model by minimizing the cost function.  We can input the features of the runners into the model to get as output a predicted running time.\n",
        "\n",
        "**2. You have more features (columns) than rows in your dataset.**\n",
        "\n",
        "Linear or logistic regresion with polynomials of low level or an SVM without kernel (also called a \"linear kernel\") to avoid overfitting an overly complex decision boundary. Regularization is important, given the small amount of data. Also Naive Bayes and K-nearest neighbors might be worth exploring. A comparative study of the error functions, given different hyperparameters for the different models, would be pertinent in order to choose one.  \n",
        "\n",
        "**3. Identify the most important characteristic predicting likelihood of being jailed before age 20.**\n",
        "\n",
        "Gradient boosting - can find variables with greatest feature importance.\n",
        "\n",
        "This can also be done with the random forest algorithm (although this would likely favor variables that ) or linear or logistic regression with lasso or ridge regularization. \n",
        "\n",
        "In all of these cases, it's important to perform feature scaling before analyzing features.  \n",
        "\n",
        "**4. Implement a filter to “highlight” emails that might be important to the recipient**\n",
        "\n",
        "Naive Bayes - will give the probability of the email being important or not based on the keywords we have included in the model. In text classification, we typically deal with huge feature spaces, such as  the number of words in the dictionary and all their possible n-gram combinations. We can then also improve the model using cross validation.\n",
        "\n",
        "**5. You have 1000+ features.**\n",
        "\n",
        "Depends on how many training examples we have? One should try using a dimensionality reduction technique like PCA in large feature spaces, even if only for exploratory data analysis. \n",
        "\n",
        "**6. Predict whether someone who adds items to their cart on a website will purchase the items.**\n",
        "\n",
        "Logistic Regression - predict whether they will or will not purchase the items. Any other binary classification method can be applied here. \n",
        "\n",
        "**7. Your dataset dimensions are 982400 x 500**\n",
        "\n",
        "Logistic Regression or SVM without kernel. I would try to use a technique that avoid overfitting. I would try to regularize any complex model (neural networks, linear regression with polynomials of high degree, etc.) \n",
        "\n",
        "**8. Identify faces in an image.**\n",
        "\n",
        "Gradient Boosting - allows us to select a subset of features from a very large number of possible features.  Combine weak learners into a more accurate model\n",
        "\n",
        "Homework: Read about using gradient boosting, decision trees, and neural networks for this task\n",
        "https://www.cc.gatech.edu/~afb/classes/CS7616-Spring2014/slides/CS7616-10-Boosting.pdf\n",
        "\n",
        "**9. Predict which of three flavors of ice cream will be most popular with boys vs girls.**\n",
        "\n",
        "KNN Classifier - Classifying which flavor of ice cream is liked by girls vs. boys - but it seems like we have two classification categories - gender and ice cream flavor (which might indicate two different targets or all possible pairings of these). We could also predict popularity (as a score, e.g., between 1 and 10) using a linear regression, given features gender, flavor, etc. \n",
        "\n",
        "Sample values for these columns: \n",
        "\n",
        "*Gender* \n",
        "M\n",
        "F\n",
        "\n",
        "*Flavor*\n",
        "Vanilla\n",
        "Chocolate\n",
        "Strawberry\n",
        "\n",
        "*Popularity* (target)\n",
        "1-10\n"
      ]
    }
  ]
}